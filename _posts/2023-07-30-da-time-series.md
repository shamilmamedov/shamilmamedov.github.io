---
layout: post
title: "Basic data augmentation methods for time series"
date: 2023-07-30 09:00:00
description: 
tags: 
categories: 
---
In recent years, supervised learning methods have shown promising results in tackling problems related to time series analysis. These problems include forecasting, classification, anomaly detection, and data imputation. Some of the recent achievements in neural networks can be attributed to the availability of large-scale datasets. However, collecting such data can be challenging when dealing with time series.

Data augmentation emerges as a method to increase the quantity of available data. This procedure aims to enhance the generalizability of models and reduce overfitting. In this article, I will delve into basic data augmentation methods specifically designed for time series analysis.

*Throughout the article, I will use the terms "time series" and "signal" interchangeably.*

# Jitering

Jittering consists of simply adding noise to time series. This technique, in addition to being one of the simplest forms of data augmentation (DA) is one of the most popular. Jittering assumes that the data are noisy which, in many cases, i.e., when dealing with sensor data, is true.

<p style="align: left; text-align:center;">
    <img src="/assets/img/blog/jittering.png" alt width="60%"/>
    <div class="caption">Figure 1. An example of jittering i.e. adding noise to a signal </div>
</p>

If the assumption holds then by adding noise we can generate new samples. Typically, Gaussian noise is added to each time step; the mean and standard deviation of this noise define the magnitude and shape of the deformation, so it is different in each application. Applying jittering to a time series results in

$$
x'(\epsilon) = \{x_1 + \epsilon_1, \dots, x_t + \epsilon_t, \dots, x_T + \epsilon_T \}
$$

where $$\epsilon \sim \mathcal{N}(0, \sigma^2)$$  stands for the additive noise at each step of the signal. Although, Gaussian noise is the most popular choice for a noise, other more complicated noise patterns such as spike, step-like trend and slope-like trend can also be utilized. Overall, the method is easy to use and safe, and often it improves the performance of a model.

# Scaling
Scaling consists of changing the magnitude of a certain step in the time series domain. The idea is to maintain the overall shape of the signal while changing its values:

$$
x'(\alpha) = \{\alpha x_1, \dots, \alpha x_t, \dots, \alpha x_T \}
$$

where $$\alpha > 0$$ defines the scale of the change. This value can be defined by a Gaussian distribution with mean 1 and with standard deviation $$\sigma$$ as a hyperparameter — $$\mathcal{N}(1, \sigma^2)$$. This method is similar to resize from the computer vision.

<p style="align: left; text-align:center;">
    <img src="/assets/img/blog/scaling.png" alt width="60%"/>
    <div class="caption">Figure 2. An example of signal scaling </div>
</p>

More advanced scaling method is magnitude warping. It consists of an application of a variable scaling to different points of the data curve. To define where to apply the transformation, a set of knots $$u = u_1,\dots,u_i$$ are defined; these knots represent points in which the scaling is performed and their values are generated by using a normal distribution. Then the magnitude of the scaling in between the knots is defined by a cubic spline interpolation of the knots $$S(x)$$. Mathematically the magnitude warping can be defined as follows:

$$
x'(\alpha) = \{\alpha_1 x_1, \dots, \alpha_t x_t,\dots, \alpha_T x_T \}
$$

where $$\alpha = \alpha_1,\dots,\alpha_i = S(x)$$.

<p style="align: left; text-align:center;">
    <img src="/assets/img/blog/mag_warping.png" alt width="60%"/>
    <div class="caption">Figure 3. An example of magnitude warping of a signal</div>
</p>

Yet another scaling technique is time warping. The idea behind time warping is very similar to magnitude warping, but the main difference between both algorithms is that Time warping modifies the curve in the temporal dimension. That is, instead of fluctuating the magnitude of the signal in each step, it stretches and shortens the time slices of the signal.

<p style="align: left; text-align:center;">
    <img src="/assets/img/blog/time_warping.png" alt width="60%"/>
    <div class="caption">Figure 4. An example of time warping of a signal</div>
</p>
Scaling methods can be very useful for medical data as people are different; the same health features can expressed differently.

# Time slicing window

Slicing, in time series, consists of cutting a portion of each data sample, to generate a different new sample.  When the original data is cropped, a different sample is produced, but unlike image processing, it is difficult to maintain all the features of the original data. New sample is

$$
x'(W) = \{x_\phi,\dots,x_t,\dots x_{\phi + W} \}
$$

where $$W$$ is the slice window that defines the crop size and $$\phi$$ is the initial point from where the slicing is performed. One of the most important drawbacks of slicing the signal is that it can lead to invalid synthetic samples because it can cut off important features of the data.

<p style="align: left; text-align:center;">
    <img src="/assets/img/blog/window_slicing.png" alt width="60%"/>
    <div class="caption">Figure 5. An example of window slicing</div>
</p>


# Rotation
Rotation can be applied to multivariate time series data by applying a rotation matrix with a defined angle. In univariate time series, rotation can be applied by flipping the data. For multivatiate time series new rotated sample is

$$
x'(R) = \{R x_1, \dots, Rx_t, \dots, Rx_T  \}
$$

where $$R$$ is the rotation matrix used to twist the data. The rotation angle $$\theta$$ can be randomly sampled from a normal distribution. This algorithm is not very usual in time series due to the fact that rotating a time series sample could make it lose the class information.

<p style="align: left; text-align:center;">
    <img src="/assets/img/blog/rotation.png" alt width="60%"/>
    <div class="caption">Figure 6. An example of rotation applied to univariate signal</div>
</p>

Be aware that applying rotation can sometimes worsen the model performance as it might change the physics of data. For example, in physical systems applying rotation might change the gravity vector yielding physically inconsistent new sample — objects instead of falling under gravity might levitate or move randomly.

# Permutation

Shuffling different time slices of data in order to perform DA is a method that generates new data patterns.  The main problem of applying permutation is that it does not preserve time
dependencies; thus, it can lead to invalid samples. Mathematically, new permuted sample can be defined as

$$
x'(w) = \{x_i,\dots,x_{i+w}, \dots, x_j, \dots, x_{j+w}, \dots, x_k, \dots, x_{k+w} \}
$$

where $$i,j,k$$ represents the first index slice of each window, so that each is selected exactly once, and $$w$$ denotes the window size.
<p style="align: left; text-align:center;">
    <img src="/assets/img/blog/permutation.png" alt width="60%"/>
    <div class="caption">Figure 7. An example of signal permutation: shuffling different window slices </div>
</p>


# Channel permutation

Changing the position of different channels in multi-dimensional data is a common practice. In computer vision, it is quite popular to swap the RGB channels to perform DA. With respect to time series, channel permutation can be applied as long as each channel of the data is still valid. The channel permutation algorithm, for multidimensional data such as $$x=\{\{x_{11},\dots,x_{1T}\}, \dots, \{x_{c1},\dots,x_{cT}\} \}$$ where $$c$$ is the number of channels, is given by

$$
x=\{\{x_{\sigma(1)1},\dots,x_{\sigma(1)T}\}, \dots, \{x_{\sigma(c)1},\dots,x_{\sigma(c)T}\} \}
$$

where $$\sigma: \{1,\dots,c \} \rightarrow \{1,\dots,c\}$$ is the used permutation of the channels.

<p style="align: left; text-align:center;">
    <img src="/assets/img/blog/channel_permutation.png" alt width="60%"/>
    <div class="caption">Figure 8. An example of channel permutation</div>
</p>


# Wrap up

In time series analysis, high-quality data is often limited. One simple approach to address this issue is by generating new data through the augmentation of existing samples.

In this article, we have introduced you to fundamental methods for augmenting time series data. Some of these techniques resemble those commonly used in computer vision, while others are specifically tailored for time series analysis, such as temporal and amplitude distortions.

The most straightforward and safe data augmentation method is **jittering** aka adding noise to the signal, which likely explains its popularity. However, you need to be careful when employing other augmentation methods; domain knowledge might be very useful for selecting appropriate augmentation method. For instance, **rotation** may seem like a harmless augmentation, but in physical systems, it alters the gravity vector. As a result, the augmented dataset could negatively impact a model trained to predict the motion of objects affected by gravity.

**Scaling methods** might prove effective in medical applications, whereas **permutation methods** may be beneficial for classification tasks but might not significantly improve regression models. In the next article, we will explore more advanced augmentation techniques, which involve discovering hidden models that generated the existing data and subsequently using these models to generate new data. Yes, we are referring to variational autoencoders and generative adversarial networks.

# Libraries
1. [Tsaug](https://github.com/arundo/tsaug) — python library for time series data augmentation
2. [tsai](https://timeseriesai.github.io/tsai/) — popular python library for time series  modeling; it has a module for data augmentation

# References
1. Images are taken from [Talavera, Edgar, et al. "Data augmentation techniques in time series domain: A survey and taxonomy." *arXiv preprint arXiv:2206.13508* (2022).](https://arxiv.org/abs/2206.13508)

2. [Wen, Qingsong, et al. "Time series data augmentation for deep learning: A survey." *arXiv preprint arXiv:2002.12478* (2020).](https://arxiv.org/abs/2002.12478)